import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
import os
from tqdm import tqdm

from dataset import SpeckleFlowDataset
from model import SpecklePINN

# ================= é…ç½® =================
CONFIG = {
    'roots': {
        'group_680W': '/data/zm/2026.1.12_testdata/1.15_150_680W/',
        'group_gaoyuzhi': '/data/zm/2026.1.12_testdata/gaoyuzhi/'
    },
    'window_size_us': 100000,
    'step_size_us': 50000,
    'batch_size': 64,
    'lr': 1e-4,
    'epochs': 50,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'lambda_flow': 1.0,
    'lambda_fit': 10.0,
    'save_dir': '/data/zm/2026.1.12_testdata/1.26_PINN_result',

    # ğŸ”¥ğŸ”¥ğŸ”¥ ä¸¥é…·éªŒè¯ï¼šä¿ç•™æµé€Ÿåˆ—è¡¨ ğŸ”¥ğŸ”¥ğŸ”¥
    # è®­ç»ƒé›†å°†çœ‹ä¸åˆ°è¿™äº›æµé€Ÿï¼Œå¿…é¡»é ç‰©ç†è§„å¾‹â€œçŒœâ€å‡ºæ¥
    'holdout_flows': [0.8, 1.8, 2.5]
}


def main():
    os.makedirs(CONFIG['save_dir'], exist_ok=True)

    # 1. å‡†å¤‡æ•°æ® (ç‰©ç†éš”ç¦»)
    print("Loading TRAIN dataset...")
    # train æ¨¡å¼ï¼šæ’é™¤ holdout_flows
    train_ds = SpeckleFlowDataset(CONFIG['roots'], mode='train',
                                  holdout_flows=CONFIG['holdout_flows'],
                                  window_size_us=CONFIG['window_size_us'],
                                  step_size_us=CONFIG['step_size_us'])

    print("Loading VAL dataset...")
    # val æ¨¡å¼ï¼šåªåŒ…å« holdout_flows
    val_ds = SpeckleFlowDataset(CONFIG['roots'], mode='val',
                                holdout_flows=CONFIG['holdout_flows'],
                                window_size_us=CONFIG['window_size_us'],
                                step_size_us=CONFIG['step_size_us'])

    train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=4)
    val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4)

    print(f"Data split: Train={len(train_ds)} slices, Val={len(val_ds)} slices")

    # 2. æ¨¡å‹
    model = SpecklePINN().to(CONFIG['device'])
    optimizer = optim.Adam(model.parameters(), lr=CONFIG['lr'])
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)

    # 3. è®­ç»ƒ
    print("Start Training (Rigorous Physics Mode)...")
    history = {'train_loss': [], 'val_loss': []}

    # å®šä¹‰ Fit Loss çš„æƒé‡ (å¯é€‰ï¼šç»™å¤´éƒ¨æ›´é«˜æƒé‡)
    # æ—¢ç„¶å½’ä¸€åŒ–ä¿®å¥½äº†ï¼Œæš‚æ—¶ç”¨å‡åŒ€æƒé‡
    # Fit loss æƒé‡ï¼šå¼ºè°ƒæ—©æœŸä¸‹é™æ®µï¼ˆä½ å…³å¿ƒçš„å‰ 1ms / 5msï¼‰
    tau_us = (model.tau_grid.detach().cpu().numpy() * 1e6).astype(np.float32)
    w = np.ones_like(tau_us, dtype=np.float32)
    w[tau_us <= 1000.0] = 5.0
    w[(tau_us > 1000.0) & (tau_us <= 5000.0)] = 2.0
    # å½’ä¸€åŒ–ï¼šè®©å¹³å‡æƒé‡ä¸º 1ï¼Œé¿å…ç­‰æ•ˆ lambda_fit çªå˜
    w = w / (np.mean(w) + 1e-9)
    fit_weights = torch.from_numpy(w).to(CONFIG['device'])

    for epoch in range(CONFIG['epochs']):
        model.train()
        total_loss = 0
        valid_batches = 0

        pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{CONFIG['epochs']}", unit="batch")

        for batch in pbar:
            g2_obs = batch['g2_curve'].to(CONFIG['device']).float()
            aux = batch['aux_input'].to(CONFIG['device']).float()
            v_label = batch['flow_label'].to(CONFIG['device']).float()
            m_val = batch['k_factor'].to(CONFIG['device']).float()

            optimizer.zero_grad()

            out = model(g2_obs, aux, m_val)

            # Loss è®¡ç®—
            g2_hat = out['g2_hat']

            # Fit Loss
            loss_fit = torch.mean(fit_weights * (g2_hat - g2_obs) ** 2)

            # Flow Loss
            v_pred = out['v_pred']
            loss_flow = torch.mean((v_pred - v_label) ** 2)

            loss = CONFIG['lambda_fit'] * loss_fit + CONFIG['lambda_flow'] * loss_flow

            if torch.isnan(loss):
                continue

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            total_loss += loss.item()
            valid_batches += 1

            pbar.set_postfix({
                'L': f"{loss.item():.2f}",
                'Fit': f"{loss_fit.item():.2f}",
                'Flow': f"{loss_flow.item():.2f}"
            })

        avg_loss = total_loss / valid_batches if valid_batches > 0 else 0.0
        history['train_loss'].append(avg_loss)

        # === éªŒè¯ ===
        model.eval()
        val_loss_sum = 0
        val_count = 0

        with torch.no_grad():
            for batch in val_loader:
                g2_obs = batch['g2_curve'].to(CONFIG['device']).float()
                aux = batch['aux_input'].to(CONFIG['device']).float()
                v_label = batch['flow_label'].to(CONFIG['device']).float()
                m_val = batch['k_factor'].to(CONFIG['device']).float()

                out = model(g2_obs, aux, m_val)
                v_err = torch.abs(out['v_pred'] - v_label).mean()

                val_loss_sum += v_err.item()
                val_count += 1

        avg_val_mae = val_loss_sum / val_count if val_count > 0 else 0.0
        history['val_loss'].append(avg_val_mae)

        scheduler.step(avg_val_mae)

        print(f"Epoch {epoch + 1} | Train Loss: {avg_loss:.4f} | Val MAE (Unseen Flows): {avg_val_mae:.4f}")

        if epoch > 0 and avg_val_mae < min(history['val_loss'][:-1]):
            torch.save(model.state_dict(), os.path.join(CONFIG['save_dir'], 'best_model.pth'))

    plt.figure(figsize=(10, 5))
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Val MAE (Holdout)')
    plt.legend()
    plt.savefig(os.path.join(CONFIG['save_dir'], 'training_result.png'))
    print("Rigorous Training Complete.")


if __name__ == "__main__":
    main()