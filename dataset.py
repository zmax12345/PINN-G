import os
import glob
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset
from scipy.signal import correlate

# 96ç‚¹ Grid (ä¿æŒä¸å˜)
TAU_LAGS = np.unique(np.concatenate([
    # 0 ~ 0.5 ms : 10 us step
    np.arange(0, 500, 10),
    # 0.5 ~ 5 ms : 100 us step
    np.arange(500, 5001, 100),
    # 5 ~ 100 ms : 1 ms step
    np.arange(5000, 100001, 1000),
])).astype(np.int64)


class SpeckleFlowDataset(Dataset):
    def __init__(self, data_roots, mode='train', holdout_flows=None, window_size_us=100000, step_size_us=50000):
        self.window_size_us = int(window_size_us)
        self.step_size_us = int(step_size_us)
        self.tau_lags = TAU_LAGS
        self.data_cache = []
        self.samples = []
        self.mode = mode
        self.holdout_flows = holdout_flows if holdout_flows is not None else []

        # è®¾å®šç§¯åˆ†æ—¶é—´ (Integration Time) ç”¨äºå°†äº‹ä»¶è½¬ä¸ºå…‰å¼ºä¿¡å·
        # 10us ä¸€ä¸ª binï¼Œè¶³ä»¥åˆ†è¾¨ 5000us çš„å»¶è¿Ÿ
        self.dt_us = 10

        print(f"Dataset ({mode}) initializing with Signal Processing Engine...")
        self._load_all_files(data_roots)
        print(f"Dataset ({mode}) initialized: {len(self.samples)} samples.")

    def _load_all_files(self, roots):
        file_idx_counter = 0
        for group_name, root_dir in roots.items():
            if not os.path.exists(root_dir):
                print(f"Warning: Directory not found: {root_dir}")
                continue

            # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šæ˜ç¡®æŒ‡å®šæ¯ä¸€ç»„çš„ m å€¼ (mm) ğŸ”¥ğŸ”¥ğŸ”¥
            # m = åƒç´ ç‰©ç†å°ºå¯¸(mm) * æ•£æ–‘åƒç´ å¤§å°(pixels)

            if 'gaoyuzhi' in group_name:
                # ç¬¬ä¸€ç»„è€æ•°æ®
                current_m = 0.012915
                print(f"   -> Group '{group_name}': Matched 'gaoyuzhi', set m = {current_m:.6f} mm")

            elif 'group_680W' in group_name or '680W' in root_dir:
                # ç¬¬äºŒç»„è€æ•°æ®
                current_m = 0.011167
                print(f"   -> Group '{group_name}': Matched '680W', set m = {current_m:.6f} mm")

            elif 'group_580W' in group_name:
                # ğŸ”¥ è¿™é‡Œå¡«ä½ æ–°æ•°æ®çš„åå­—å’Œç®—å‡ºæ¥çš„ m å€¼
                # ä¾‹å¦‚ï¼šåƒç´  0.00345mm * æ•£æ–‘ 3.2px = 0.01104
                current_m = 0.011808  # <--- è¯·ä¿®æ”¹è¿™é‡Œï¼
                print(f"   -> Group '{group_name}': Matched 'new_experiment', set m = {current_m:.6f} mm")

            files = glob.glob(os.path.join(root_dir, "*.csv"))
            for fpath in files:
                try:
                    fname = os.path.basename(fpath)
                    try:
                        name_clean = fname.replace("_clip.csv", "").replace("mm.csv", "").replace("mm", "")
                        flow_val = float(name_clean)
                    except:
                        continue

                    # ä¸¥æ ¼åˆ’åˆ†
                    is_holdout = False
                    for hv in self.holdout_flows:
                        if abs(flow_val - hv) < 0.01:
                            is_holdout = True
                            break

                    if self.mode == 'train' and is_holdout: continue
                    if self.mode == 'val' and not is_holdout: continue

                    with open(fpath, 'r', encoding='utf-8', errors='ignore') as f:
                        df = pd.read_csv(f, header=None, usecols=[0, 1, 2], dtype=str, engine='c', on_bad_lines='skip')
                    df = df.apply(pd.to_numeric, errors='coerce').dropna().astype(np.int64)
                    max_vals = df.max().values
                    tin_col_idx = np.argmax(max_vals)
                    tin_array = np.ascontiguousarray(df.iloc[:, tin_col_idx].sort_values().values)

                    if len(tin_array) < 1000: continue
                    duration = tin_array[-1] - tin_array[0]
                    if duration > 60 * 1e6 or duration <= 0: continue

                    self.data_cache.append(tin_array)
                    self._make_slices_fast(file_idx_counter, tin_array, flow_val, current_m)
                    file_idx_counter += 1

                except Exception as e:
                    print(f"Skip {fpath}: {e}")

    def _make_slices_fast(self, file_idx, t_all, label, m_val):
        t_min, t_max = t_all[0], t_all[-1]
        start_times = np.arange(t_min, t_max - self.window_size_us + 1, self.step_size_us)
        if len(start_times) == 0: return

        end_times = start_times + self.window_size_us
        idx_starts = np.searchsorted(t_all, start_times)
        idx_ends = np.searchsorted(t_all, end_times)
        counts = idx_ends - idx_starts

        valid_mask = counts > 1000  # è‡³å°‘è¦æœ‰äº‹ä»¶
        for i in np.where(valid_mask)[0]:
            self.samples.append((file_idx, int(idx_starts[i]), int(idx_ends[i]), np.float32(label), np.float32(m_val)))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        file_idx, start, end, label, m_val = self.samples[idx]
        ts = self.data_cache[file_idx][start:end]
        ts = ts - ts[0]  # å½’é›¶

        # === ğŸ”¥ æ ¸å¿ƒé‡æ„ï¼šåŸºäº FFT çš„æ ‡å‡†å…‰å¼ºè‡ªç›¸å…³ ===
        # 1. è½¬ä¸ºå…‰å¼ºä¿¡å· (Intensity Trace)
        # çª—å£æ€»æ—¶é•¿ window_size_usï¼Œåˆ†è¾¨ç‡ dt_us
        num_bins = self.window_size_us // self.dt_us
        # ä½¿ç”¨ç›´æ–¹å›¾ç»Ÿè®¡æ¯ä¸ª dt å†…çš„äº‹ä»¶æ•° -> I(t)
        I_t, _ = np.histogram(ts, bins=num_bins, range=(0, self.window_size_us))
        I_t = I_t.astype(np.float32)

        # 2. è®¡ç®—è‡ªç›¸å…³ G2(\tau) = <I(t)I(t+\tau)>
        # ä½¿ç”¨ FFT åŠ é€Ÿå·ç§¯ï¼šCorrelate I_t with itself
        # mode='full' è¿”å›é•¿åº¦ 2*N-1ï¼Œä¸­å¿ƒæ˜¯ 0 æ»å
        acf = correlate(I_t, I_t, mode='full')

        # å–å³åŠéƒ¨åˆ† (æ­£æ»å)
        center = len(acf) // 2
        acf_right = acf[center:]  # é•¿åº¦ num_bins

        # 3. å½’ä¸€åŒ–ï¼šg2 = <I(t)I(t+\tau)> / <I(t)>^2
        # æ³¨æ„ï¼šcorrelate æ˜¯æ±‚å’Œä¸æ˜¯æ±‚å¹³å‡ï¼Œæ‰€ä»¥è¦é™¤ä»¥é‡å çš„ bin æ•°é‡
        normalization_array = np.arange(num_bins, 0, -1).astype(np.float32)
        G2 = acf_right / (normalization_array + 1e-9)  # G2(\tau) raw

        mean_I = np.mean(I_t)
        baseline = mean_I ** 2

        if baseline > 1e-9:
            g2_final = G2 / baseline
        else:
            g2_final = np.ones_like(G2)

        # 4. æ˜ å°„åˆ°æˆ‘ä»¬çš„ TAU_LAGS ç½‘æ ¼
        # TAU_LAGS å•ä½æ˜¯ usï¼Œæˆ‘ä»¬çš„ dt_us æ˜¯ 10us
        # æ‰€ä»¥ index = tau / 10
        indices = (self.tau_lags // self.dt_us).astype(np.int64)
        indices = np.clip(indices, 0, len(g2_final) - 1)

        g2_feature = g2_final[indices]

        # 5. ç®€å•æ¸…æ´—
        g2_feature = np.nan_to_num(g2_feature, nan=1.0)
        # ç‰©ç†ä¸Š g2 é€šå¸¸ä» >1 å¼€å§‹è¡°å‡åˆ° 1ã€‚
        # å¦‚æœå™ªå£°å¯¼è‡´ < 0.5ï¼Œè§†ä¸ºå¼‚å¸¸
        g2_feature = np.maximum(g2_feature, 0.5)

        # Aux input (Mean Intensity)
        log_intensity = np.log10(mean_I + 1e-6).astype(np.float32)

        return {
            'g2_curve': torch.from_numpy(g2_feature),
            'aux_input': torch.tensor([log_intensity]),
            'flow_label': torch.tensor([label]),
            'k_factor': torch.tensor([m_val])
        }